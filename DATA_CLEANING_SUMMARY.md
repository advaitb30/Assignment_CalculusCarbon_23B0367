# Data Cleaning & Integration Summary
## Calculus Carbon Assignment - Phase 1 & 2

---

## ğŸ¯ Objective

Transform 4 messy, inconsistent datasets into a unified, queryable structure suitable for AI-powered analysis.

---

## ğŸ“Š Dataset Overview

### Input Datasets
| Dataset | Records | Key Issues Identified |
|---------|---------|----------------------|
| **Project Developers** | 15 | Name variations, numeric formatting, missing Hectares |
| **Investors** | 12 | Multi-language text, no duplicate detection needed |
| **Emails** | 20 | Entity mentions scattered, date parsing needed |
| **Transcripts** | 10 | Unstructured text, entity extraction required |

---

## ğŸ”§ Data Quality Issues & Solutions

### 1. **Name Variations** âœ…

**Problem:**
- Same entity with multiple spellings
- Example: "VerdeNova Solutions" = "Verdenova" = "Verde Nova"

**Solution:**
- Created canonical entity IDs for each developer/investor
- Built alternate name mappings
- Used fuzzy matching (RapidFuzz) with 85% threshold
- Preserved all variations in metadata

**Code Location:** `04_entity_resolution.py`

**Output:** `developer_entity_map.json`, `investor_entity_map.json`

---

### 2. **Numeric Formatting Issues** âœ…

**Problem:**
- Numbers stored as strings: "1,200" vs 1200
- Mixed types in same column
- "NA" vs actual null values

**Solution:**
```python
def clean_numeric(value):
    # Remove commas
    # Convert "NA" to None
    # Handle int vs float
    # Return None for invalid values
```

**Examples:**
- "1,200" â†’ 1200 (int)
- "850" â†’ 850 (int)
- "NA" â†’ None
- "3,400" â†’ 3400 (int)

**Code Location:** `03_data_cleaning.py`

---

### 3. **Multi-language Content** âœ…

**Problem:**
- Investor mandates mixed Spanish and English
- Example: "NorthStar busca financiar proyectos forestales..."

**Solution:**
- Detected language mixing using keyword analysis
- Preserved original text (no translation)
- Flagged for context in retrieval system
- Used `unidecode` for encoding issues

**Decision:** Keep original text, handle in LLM prompt layer

**Code Location:** `02_data_quality_report.py`

---

### 4. **Missing Values** âœ…

**Problem:**
- 1 developer missing "Hectares"
- Some optional fields empty

**Solution:**
- **Did NOT impute missing values arbitrarily**
- Flagged missing data explicitly
- Preserved as None/null in cleaned data
- Let query system handle gracefully

**Reasoning:** 
Better to acknowledge data gaps than introduce errors through imputation.

---

### 5. **Entity Cross-References** âœ…

**Problem:**
- Emails mention "VerdeNova" and "NorthStar" but no structured links
- Transcripts reference projects by name only

**Solution:**
- NLP-based entity extraction from text
- Pattern matching for entity names
- Created `entity_relationships` table
- Tracked mention source (email ID, transcript ID)

**Example:**
```
Email E001 mentions:
- Developer: VerdeNova Solutions (D001)
- Generates relationship: E001 â†’ D001
```

**Code Location:** `04_entity_resolution.py`, `05_data_integration.py`

---

### 6. **Country Name Standardization** âœ…

**Problem:**
- "Brasil" vs "Brazil"
- "Peru" vs "PerÃº"

**Solution:**
```python
country_map = {
    'brasil': 'Brazil',
    'perÃº': 'Peru',
    # ...
}
```

**Code Location:** `03_data_cleaning.py`

---

### 7. **Project Type Normalization** âœ…

**Problem:**
- "ARR" vs "A&R" vs "arr"

**Solution:**
- Standardized to uppercase abbreviations
- Mapped variations to canonical forms

**Code Location:** `03_data_cleaning.py`

---

## ğŸ“ Output Data Structures

### 1. **Master Entities Table**
Unified view of all entities (developers + investors)

**Schema:**
```
entity_id | entity_type | canonical_name | alternate_names | primary_contact | email | country | metadata
```

**Purpose:** Single source of truth for all entities

---

### 2. **Enriched Projects Table**
Enhanced developer data with metadata

**Additions:**
- `alternate_names_list`: Parsed alternate names
- `last_updated`: Timestamp
- Original fields preserved

**Purpose:** Queryable project database

---

### 3. **Unified Communications Table**
Emails + Transcripts combined

**Schema:**
```
communication_id | communication_type | date | from | to | subject | body | mentioned_developers | mentioned_investors
```

**Key Feature:** Entity mentions stored as JSON arrays for easy lookup

**Purpose:** Full conversation history with entity tracking

---

### 4. **Entity Relationships Table**
Who's connected to whom

**Schema:**
```
entity_1 | entity_1_name | entity_2 | entity_2_name | relationship_type | source_type | source_id
```

**Relationship Types:**
- `mentioned_together`: Found in same email/transcript
- `prior_interaction`: From investor notes

**Purpose:** Graph-like relationship mapping

---

## ğŸ¯ Key Design Decisions

### âœ… Preserved Original Data
- Never deleted source fields
- Cleaned versions added as new columns where needed
- Full audit trail maintained

### âœ… Explicit Null Handling
- Distinguished between missing (None) and "NA" strings
- No arbitrary imputation
- Flags for downstream handling

### âœ… Fuzzy Matching Threshold: 85%
- **Too low (70%):** False positives
- **Too high (95%):** Misses variations
- **85%:** Sweet spot for this data

### âœ… Entity-Centric Design
- Everything links back to canonical entity IDs
- Makes querying and relationships straightforward

### âœ… JSON for Complex Fields
- Entity mentions stored as JSON arrays
- Metadata stored as JSON objects
- Easy to parse, filter, and query

---

## ğŸ“ˆ Statistics

### Before Cleaning
- 4 separate datasets
- 57 total records
- Name variations: 15
- Numeric issues: ~8 instances
- Missing values: ~1% of fields

### After Cleaning & Integration
- 4 unified tables
- 27 unique entities
- 30+ entity relationships discovered
- 100% consistent formatting
- All cross-references linked

---

## ğŸ” Data Provenance

Every piece of information tracks back to its source:

```python
# Example: Entity relationship
{
    "entity_1": "D001",
    "entity_2": "I001", 
    "source_type": "email",
    "source_id": "E001"
}
```

This enables:
- Verification of claims
- Context retrieval
- Confidence scoring

---

## âš ï¸ Known Limitations

1. **Language Detection**: Simple keyword-based (could use langdetect library for better accuracy)

2. **Entity Extraction**: Pattern matching only (could enhance with spaCy NER)

3. **Date Parsing**: Basic pandas date parsing (some formats might fail)

4. **Fuzzy Threshold**: Fixed at 85% (could make dynamic per entity type)

5. **No Geocoding**: Countries as text only (could add lat/long)

---

## ğŸš€ Ready for Phase 3

With cleaned, integrated data, we can now:

âœ… Build semantic search over communications  
âœ… Create structured filters (region, sector, ticket size)  
âœ… Answer multi-source queries  
âœ… Generate context-aware summaries  
âœ… Track entity relationships  

---

## ğŸ“ Files Generated

```
reports/
â”œâ”€â”€ 01_data_exploration_report.json      # Initial profiling
â””â”€â”€ 02_data_quality_issues.json         # Detailed issues

data/cleaned/
â”œâ”€â”€ developers_cleaned.csv              # Standardized developers
â”œâ”€â”€ investors_cleaned.csv               # Standardized investors
â”œâ”€â”€ emails_cleaned.csv                  # Cleaned emails
â””â”€â”€ transcripts_cleaned.csv             # Cleaned transcripts

data/integrated/
â”œâ”€â”€ master_entities.csv                 # Unified entities
â”œâ”€â”€ enriched_projects.csv               # Enhanced projects
â”œâ”€â”€ unified_communications.csv          # All communications
â”œâ”€â”€ entity_relationships.csv            # Relationship graph
â”œâ”€â”€ developer_entity_map.json           # Name mappings
â”œâ”€â”€ investor_entity_map.json            # Name mappings
â”œâ”€â”€ developer_reverse_lookup.json       # Variant â†’ ID lookup
â”œâ”€â”€ investor_reverse_lookup.json        # Variant â†’ ID lookup
â”œâ”€â”€ email_entity_mentions.json          # Email entity tracking
â”œâ”€â”€ transcript_entity_mentions.json     # Transcript entity tracking
â””â”€â”€ integration_summary.json            # Summary stats
```

---

## ğŸ“ Assignment Requirements Met

âœ… **Data Integration**: All 4 datasets merged with proper reconciliation  
âœ… **Duplicate Handling**: Fuzzy matching with documented threshold  
âœ… **Missing Data**: Explicitly handled with no arbitrary imputation  
âœ… **Inconsistent Naming**: Entity resolution with canonical IDs  
âœ… **Unique Identification**: Every entity has unique, stable ID  
âœ… **Documented Reasoning**: Code comments + this summary  

---

**Total Processing Time:** ~5-10 seconds  
**Lines of Code:** ~1000 (across 5 scripts)  
**Data Quality Improvement:** Messy â†’ Query-ready

---

## ğŸ¤ Next Steps for Phase 3

1. Load integrated tables
2. Create vector embeddings for text fields
3. Build hybrid search (structured + semantic)
4. Integrate Groq API for LLM queries
5. Create Streamlit interface
6. Test with sample questions

**Ready to build the AI query system!** ğŸš€